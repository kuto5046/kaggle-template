{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys \n",
    "import os \n",
    "import logzero \n",
    "import wandb \n",
    "import pickle \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import noglobal, pickle_load, pickle_save, HydraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    version = '001'\n",
    "    comment = 'test'\n",
    "    input_dir = '/home/user/work/input/wherethereiscodethereisbug'\n",
    "    output_dir = f'/home/user/work/output/{version}' \n",
    "    seed = 42\n",
    "    target_col = 'label'\n",
    "    wandb_init = {\n",
    "        \"project\": \"debug\",\n",
    "        \"entity\": \"kuto5046\",\n",
    "        \"group\": f\"exp{version}\",\n",
    "        \"dir\": output_dir,\n",
    "        \"tags\": [],\n",
    "        \"mode\": \"disabled\", \n",
    "    }\n",
    "    n_splits = 5\n",
    "    use_fold = [0]  # fold1つで終える場合[0], 全てのfoldを実行する場合[0,1,2,3,4]\n",
    "\n",
    "    # model設定読み込み\n",
    "    model_config_name = 'lgb_binary'  # タスクや使うモデルに応じて変更\n",
    "    model_config = HydraConfig.get_cnf(config_path='/home/user/work/configs/model/', config_name=model_config_name)\n",
    "    num_boost_round = model_config['num_boost_round']\n",
    "    model_name = model_config.name\n",
    "    model_params = dict(model_config['params'])\n",
    "\n",
    "    \n",
    "\n",
    "c = Config()\n",
    "# c = HydraConfig.get_cnf(config_path='/home/user/work/configs/', config_name='config.yaml')\n",
    "os.makedirs(c.output_dir, exist_ok=True)\n",
    "logger = logzero.setup_logger(name='main', logfile=f'{c.output_dir}/result.log', level=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 念の為check\n",
    "c.model_name, c.num_boost_round, c.model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{c.input_dir}/train.csv')\n",
    "test = pd.read_csv(f'{c.input_dir}/test.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.base import Feature, generate_features, get_categorical_col, get_numerical_col, load_datasets\n",
    "from src.features.encoder import count_encoder, ordinal_encoder, pp_for_categorical_encoding, target_encoder\n",
    "from src.features.nlp import count_lda_vectorize, tfidf_svd_vectorize, UniversalSentenceEncoder, BertSequenceVectorizer, Sentence2Vec, SCDVEmbedder, get_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = pd.concat([train, test]).reset_index(drop=True)\n",
    "whole = pd.concat([whole, whole['code'].str.split('\\n', expand=True).add_prefix('code_')], axis=1)\n",
    "\n",
    "for i in range(5):\n",
    "    whole[f'code_{i}'] = whole[f'code_{i}'].str.strip()\n",
    "\n",
    "cat_cols = get_categorical_col(whole, skip_cols=['id', c.target_col])\n",
    "numerical_cols = get_numerical_col(whole, skip_cols=['id', c.target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = whole[~whole[c.target_col].isna()].reset_index(drop=True)\n",
    "test = whole[whole[c.target_col].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = pp_for_categorical_encoding(train, test, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TfidfSvdCode2(Feature):\n",
    "#     def create_features(self):\n",
    "#         col = 'code_2'\n",
    "#         self.train = tfidf_svd_vectorize(train, col=col)\n",
    "#         self.test = tfidf_svd_vectorize(test, col=col)\n",
    "\n",
    "# class CoundLDACode2(Feature):\n",
    "#     def create_features(self):\n",
    "#         col = 'code_2'\n",
    "#         self.train = count_lda_vectorize(train, col=col)\n",
    "#         self.test = count_lda_vectorize(test, col=col)\n",
    "\n",
    "\n",
    "# class OrdinalEncode(Feature):\n",
    "#     def create_features(self):\n",
    "#         self.train, self.test = ordinal_encoder(train, test, cat_cols)\n",
    "\n",
    "\n",
    "# class BertVecCode2(Feature):\n",
    "#     def create_features(self):\n",
    "#         bert = BertSequenceVectorizer()\n",
    "#         col = 'code_2'\n",
    "#         self.train = bert.vectorize_to_df(train, col)\n",
    "#         self.test = bert.vectorize_to_df(test, col)\n",
    "\n",
    "\n",
    "# class USEncodeCode2(Feature):\n",
    "#     def create_features(self):\n",
    "#         col = 'code_2'\n",
    "#         usencoder = UniversalSentenceEncoder()\n",
    "#         self.train = usencoder.vectorize(train, col)\n",
    "#         self.test = usencoder.vectorize(test, col)\n",
    "\n",
    "\n",
    "class Sentence2VecCode2(Feature):\n",
    "    def create_features(self):\n",
    "        col = 'code_2'\n",
    "        ndim = 160 \n",
    "        encoder = Sentence2Vec(model_file='/home/user/work/input/resource/160/wikipedia-160.txt')\n",
    "        self.train = encoder.vectorize_to_df(train, col, ndim)\n",
    "        self.test = encoder.vectorize_to_df(test, col, ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = 'pickle'\n",
    "generate_features(globals(), ext=ext, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\n",
    "    # 'TfidfSvdCode2',\n",
    "    # 'OrdinalEncode',\n",
    "    # 'CoundLDACode2',\n",
    "    # 'USEncodeCode2',\n",
    "    # 'BertVecCode2',\n",
    "    'Sentence2VecCode2'\n",
    "]\n",
    "train_data, test_data = load_datasets(feats, ext=ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = get_categorical_col(train_data)\n",
    "cat_cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用する特徴量&label\n",
    "for f in train_data.columns:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[c.target_col] = train[c.target_col].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cv import get_kfold, get_stratifiedkfold, get_groupkfold\n",
    "cv = get_stratifiedkfold(train_data, c.target_col, n_splits=5)\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.gbdt import get_callbacks\n",
    "callbacks = get_callbacks(c.model_name)\n",
    "callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.gbdt import get_model # , LGBModel, XGBModel, CBModel \n",
    "\n",
    "model = get_model(c.model_name, c.model_params, c.num_boost_round, cat_cols, c.output_dir, callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "def calc_score(true, pred):\n",
    "    return roc_auc_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(model, train, test, cv, config, cat_cols, target_col):\n",
    "    oofs = []\n",
    "    preds = []\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv):\n",
    "        wandb.init(**config.wandb_init, name=f'exp{config.version}-fold{i}')\n",
    "\n",
    "        if i not in [0]:\n",
    "            break \n",
    "\n",
    "        logger.info(\"############\")\n",
    "        logger.info(f\"fold {i}\")\n",
    "        logger.info(\"############\")\n",
    "\n",
    "        _train = train.loc[idx_train].reset_index(drop=True)\n",
    "        _valid = train.loc[idx_valid].reset_index(drop=True)\n",
    "\n",
    "        # target encoding\n",
    "        # for col in cat_cols:\n",
    "        #     _train, _valid = target_encoder(_train, _valid, col, target_col)\n",
    "        #     _, test = target_encoder(train, test, col, target_col)\n",
    "\n",
    "        X_train = _train.drop(target_col, axis=1)\n",
    "        y_train = _train[target_col]\n",
    "        X_valid = _valid.drop(target_col, axis=1)\n",
    "        y_valid = _valid[target_col]\n",
    "        X_test = test\n",
    "\n",
    "        model.train(X_train, y_train, X_valid, y_valid)\n",
    "        model.save(i)\n",
    "        pred = model.predict(X_valid)\n",
    "\n",
    "        # evaluate\n",
    "        score = calc_score(y_valid, pred)\n",
    "        logger.info(f'fold-{i} score: {score}')\n",
    "        wandb.log({'CV': score})\n",
    "\n",
    "        # create oof\n",
    "        oof_df = pd.DataFrame(pred, index=idx_valid)\n",
    "        oofs.append(oof_df)\n",
    "\n",
    "        # pred\n",
    "        pred_test = model.predict(X_test)\n",
    "        np.save(f\"{c.output_dir}/pred_test_{i}\", pred_test)\n",
    "        preds.append(pred_test)\n",
    "\n",
    "        if i!=len(cv)-1:\n",
    "            wandb.finish()\n",
    "\n",
    "    # oofを保存\n",
    "    oof = np.array(pd.concat(oofs).sort_index())\n",
    "    np.save(f\"{c.output_dir}/oof\", oof)\n",
    "    return model, oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, oof = train_pipeline(model, train_data, test_data, cv, c, cat_cols, c.target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import plot_importance\n",
    "# catboostは対応していない\n",
    "plot_importance(model.models, output_dir=c.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(len(cv)):\n",
    "    pred = np.load(f'{c.output_dir}/pred_test_{i}.npy')\n",
    "    preds.append(pred)\n",
    "pred_test = np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train[c.target_col], label='train')\n",
    "sns.distplot(pred_test, label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(f'{c.input_dir}/sample_submission.csv')\n",
    "sub['label'] = pred_test\n",
    "sub.to_csv(f'{c.output_dir}/submission_exp{c.version}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
