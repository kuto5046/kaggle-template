{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys \n",
    "import os \n",
    "import pickle \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from pathlib import Path\n",
    "import wandb\n",
    "sns.set()\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.utils import pickle_load, pickle_save, seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    version = '039'\n",
    "    comment = 'cat'\n",
    "    input_dir = Path('../input/atmaCup15_dataset/')\n",
    "    cv_strategy = ['group', 'stratified'][1]\n",
    "    output_root_dir = Path(f'../output/{version}')\n",
    "    output_dir = output_root_dir / f'{cv_strategy}'\n",
    "    feature_dir = Path(f'../features/{cv_strategy}')\n",
    "    seed = 42\n",
    "    target_col = 'score'\n",
    "    wandb_init = {\n",
    "        \"project\": \"atma15\",\n",
    "        \"entity\": \"kuto5046\",\n",
    "        \"group\": f\"exp{version}\",\n",
    "        \"dir\": output_dir,\n",
    "        \"tags\": [],\n",
    "        \"mode\": \"disabled\", \n",
    "    }\n",
    "    n_splits = 5\n",
    "    use_fold = [0,1,2,3,4]  # fold1つで終える場合[0], 全てのfoldを実行する場合[0,1,2,3,4]\n",
    "\n",
    "    # model設定読み込み\n",
    "    model_config_name = 'cb_regression'  # タスクや使うモデルに応じて変更\n",
    "    model_config = HydraConfig.get_cnf(config_path='../configs/model/', config_name=model_config_name)\n",
    "    num_boost_round = model_config['num_boost_round']\n",
    "    model_name = model_config.name\n",
    "    model_params = dict(model_config['params'])\n",
    "\n",
    "\n",
    "c = Config()\n",
    "# c = HydraConfig.get_cnf(config_path='/home/user/work/configs/', config_name='config.yaml')\n",
    "c.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "c.feature_dir.mkdir(parents=True, exist_ok=True)\n",
    "seed_everything(c.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "class StreamToLogger:\n",
    "    def __init__(self, logger, level):\n",
    "        self.logger = logger\n",
    "        self.level = level\n",
    "\n",
    "    def write(self, message):\n",
    "        if message.rstrip() != \"\":\n",
    "            self.logger.log(self.level, message.rstrip())\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "def get_logger(output_dir:Path):\n",
    "    logger = logging.getLogger('main')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # File handler for outputting to a log file\n",
    "    file_handler = logging.FileHandler(output_dir / 'result.log')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Stream handler for outputting to console\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "\n",
    "    # Redirect stdout and stderr\n",
    "    # sys.stdout = StreamToLogger(logger, logging.INFO)\n",
    "    # sys.stderr = StreamToLogger(logger, logging.ERROR)\n",
    "\n",
    "    return logger \n",
    "\n",
    "logger = get_logger(c.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cの中身を表示\n",
    "for k, v in Config.__dict__.items():\n",
    "    logger.info(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = pd.read_csv(c.input_dir /'train.csv')\n",
    "test = pd.read_csv(c.input_dir / 'test.csv')\n",
    "anime = pd.read_csv(c.input_dir / 'anime.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_only_anime_ids = set(train['anime_id']) - set(test['anime_id'])\n",
    "test_only_anime_ids = set(test['anime_id']) - set(train['anime_id'])\n",
    "train_only_user_ids = set(train['user_id']) - set(test['user_id'])\n",
    "test_only_user_ids = set(test['user_id']) - set(train['user_id'])\n",
    "print(f'train_only_anime_ids: {len(train_only_anime_ids)}')\n",
    "print(f'test_only_anime_ids: {len(test_only_anime_ids)}')\n",
    "print(f'train_only_user_ids: {len(train_only_user_ids)}')\n",
    "print(f'test_only_user_ids: {len(test_only_user_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.query('anime_id not in @train_only_anime_ids and user_id not in @train_only_user_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.cluster.hierarchy import DisjointSet\n",
    "import Levenshtein\n",
    "\n",
    "def get_original_work_name(df, threshold=0.3):\n",
    "\n",
    "    _feature = df.japanese_name.tolist()\n",
    "    _n = df.shape[0]\n",
    "\n",
    "    _disjoint_set = DisjointSet(list(range(_n)))\n",
    "    for i, j in combinations(range(_n), 2):\n",
    "        if _feature[i] is np.nan or _feature[j] is np.nan:\n",
    "            lv_dist, jw_dist = 0.5, 0.5\n",
    "        else:\n",
    "            lv_dist = 1 - Levenshtein.ratio(_feature[i], _feature[j])\n",
    "            jw_dist = 1 - Levenshtein.jaro_winkler(_feature[i], _feature[j])\n",
    "        _d = (lv_dist + jw_dist) / 2\n",
    "\n",
    "        if _d < threshold:\n",
    "            _disjoint_set.merge(i, j)\n",
    "\n",
    "    _labels = [None] * _n\n",
    "    for subset in _disjoint_set.subsets():\n",
    "        label = _feature[list(subset)[0]]\n",
    "        for element in subset:\n",
    "            _labels[element] = label\n",
    "    df[\"original_work_name\"] = _labels\n",
    "    return df\n",
    "\n",
    "\n",
    "anime[\"japanese_name\"] = anime[\"japanese_name\"].apply(lambda x:np.nan if x==\"Unknown\" else x)\n",
    "anime = get_original_work_name(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime['episodes'] = anime['episodes'].replace('Unknown', np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = pd.concat([train, test]).reset_index(drop=True)\n",
    "whole = whole.merge(anime, on='anime_id', how='left')\n",
    "whole.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = get_categorical_col(whole, skip_cols=['id', c.target_col])\n",
    "numerical_cols = get_numerical_col(whole, skip_cols=['id', c.target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = whole[~whole[c.target_col].isna()].reset_index(drop=True).copy()\n",
    "test = whole[whole[c.target_col].isna()].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cv import get_kfold, get_stratifiedkfold, get_groupkfold, get_fold, split_train_valid\n",
    "if c.cv_strategy == 'group':\n",
    "    cv = get_groupkfold(train, c.target_col, 'user_id', n_splits=5)\n",
    "elif c.cv_strategy == 'stratified':\n",
    "    cv = get_stratifiedkfold(train, c.target_col, n_splits=5)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "train = get_fold(train, cv)\n",
    "fold_list = sorted(list(train['fold'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.base import Feature, generate_features, get_categorical_col, get_numerical_col, load_datasets\n",
    "from src.features.encoder import count_encoder, ordinal_encoder, pp_for_categorical_encoding, target_encoder\n",
    "from src.features.nlp import count_lda_vectorize, tfidf_svd_vectorize, UniversalSentenceEncoder, BertSequenceVectorizer, Sentence2Vec, SCDVEmbedder, get_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import random\n",
    "import gensim.downloader\n",
    "from gensim.models import word2vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from surprise import (\n",
    "    NormalPredictor, BaselineOnly, KNNBasic, KNNWithMeans, KNNWithZScore,\n",
    "    KNNBaseline, SVD, SVDpp, NMF, SlopeOne, CoClustering\n",
    ")\n",
    "from surprise import Dataset, Reader\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class Target(Feature):\n",
    "    def create_features(self, fold:Optional[int]=None):\n",
    "        if fold is not None:\n",
    "            # for cv\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            self.train = _train[[c.target_col]]\n",
    "            self.valid = _valid[[c.target_col]]\n",
    "        else:\n",
    "            # for submission\n",
    "            self.train = train[[c.target_col]].copy()\n",
    "            self.test = pd.DataFrame(np.zeros(len(test)), columns=[c.target_col])  # dummy\n",
    "\n",
    "        \n",
    "class Numerical(Feature):\n",
    "    def create_features(self, fold:Optional[int]=None):\n",
    "        if fold is not None:\n",
    "            # for cv\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            self.train = _train[numerical_cols]\n",
    "            self.valid = _valid[numerical_cols]\n",
    "        else:\n",
    "            # for submission\n",
    "            self.train = train[numerical_cols].copy()\n",
    "            self.test = test[numerical_cols].copy()\n",
    "\n",
    "\n",
    "class OrdinalEncode(Feature):\n",
    "    def ordinal_encoder(self, train:pd.DataFrame, test:pd.DataFrame, cols:list[str], prefix:str):\n",
    "        encoder = OrdinalEncoder()\n",
    "        _whole = pd.concat([train, test], axis=0).reset_index(drop=True)\n",
    "        encoder.fit(_whole[cols])\n",
    "        _train = pd.DataFrame(encoder.transform(train[cols]), columns=cols).add_prefix(prefix).astype('category')\n",
    "        _test = pd.DataFrame(encoder.transform(test[cols]), columns=cols).add_prefix(prefix).astype('category')\n",
    "        return _train, _test\n",
    "\n",
    "    def create_features(self, fold: Optional[int]=None):\n",
    "        if c.cv_strategy == 'group':\n",
    "            use_cols = [\n",
    "                # 'user_id',\n",
    "                # 'anime_id',\n",
    "                'genres',\n",
    "                # 'japanese_name',\n",
    "                'type',\n",
    "                'aired',\n",
    "                'producers',\n",
    "                'licensors',\n",
    "                'studios',\n",
    "                'source',\n",
    "                'duration',\n",
    "                'rating',\n",
    "                # 'original_work_name'\n",
    "            ]\n",
    "        else:\n",
    "            use_cols = [\n",
    "                'user_id',\n",
    "                'anime_id',\n",
    "                'genres',\n",
    "                # 'japanese_name',\n",
    "                'type',\n",
    "                'aired',\n",
    "                'producers',\n",
    "                'licensors',\n",
    "                'studios',\n",
    "                'source',\n",
    "                'duration',\n",
    "                'rating',\n",
    "                'original_work_name'\n",
    "            ]\n",
    "        prefix= 'ordinal_enc'\n",
    "        _train, _test = self.ordinal_encoder(train, test, use_cols, prefix)\n",
    "        \n",
    "        if fold is not None:\n",
    "            _train['fold'] = train['fold'].to_numpy()\n",
    "            _train, _valid = split_train_valid(_train, fold)\n",
    "            self.train = _train.filter(like=prefix)\n",
    "            self.valid = _valid.filter(like=prefix)\n",
    "        else:\n",
    "            self.train = _train.copy()\n",
    "            self.test = _test.copy()\n",
    "\n",
    "\n",
    "class CountEncode(Feature):\n",
    "    def count_encoder(self, df, cols):\n",
    "        _df = df.copy()\n",
    "        prefix = 'count_enc'\n",
    "        for col in cols:\n",
    "            encoder = whole[col].value_counts()\n",
    "            _df[f'{prefix}_{col}'] = _df[col].map(encoder)\n",
    "        return _df\n",
    "\n",
    "    def create_features(self, fold:Optional[int]=None):\n",
    "        \"\"\" \n",
    "        どれくらいよく見られているか\n",
    "        testを含めたcountはtestの予測でも使えるので使う\n",
    "        validの時はvalidを含めたcountを使う\n",
    "        \"\"\"\n",
    "        use_cols = [\n",
    "            'user_id',\n",
    "            'anime_id',\n",
    "            'original_work_name',\n",
    "            ]\n",
    "        \n",
    "        if fold is not None:\n",
    "            _train = self.count_encoder(train, use_cols)\n",
    "            _train, _valid = split_train_valid(_train, fold)\n",
    "            self.train = _train.filter(like='count_enc')\n",
    "            self.valid = _valid.filter(like='count_enc')\n",
    "        else:\n",
    "            whole = pd.concat([train, test])\n",
    "            _whole = self.count_encoder(whole, use_cols)\n",
    "            _train = _whole[~whole['score'].isna()].sort_index()\n",
    "            _test = _whole[whole['score'].isna()].sort_index()\n",
    "            self.train = _train.filter(like='count_enc')\n",
    "            self.test = _test.filter(like='count_enc')\n",
    "  \n",
    "\n",
    "class TargetEncode(Feature):\n",
    "    def create_features(self, fold:Optional[int]=None):\n",
    "        if c.cv_strategy == 'group':\n",
    "            \"\"\"\n",
    "            unseen userを学習するのでuser_idは使わない\n",
    "            \"\"\"\n",
    "            use_cols = []\n",
    "        elif c.cv_strategy == 'stratified':\n",
    "            use_cols = ['user_id', 'anime_id', 'original_work_name']\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if fold is not None:\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            self.train, self.valid = target_encoder(_train, _valid, use_cols, c.target_col, methods=['mean', 'std'])\n",
    "        else:\n",
    "            self.train, self.test = target_encoder(train, test, use_cols, c.target_col, methods=['mean', 'std'])\n",
    "\n",
    "\n",
    "class MultiTagEncode(Feature):\n",
    "    @staticmethod\n",
    "    def create_one_hot_and_svd_features(anime, multilabel_cols:list[str]):\n",
    "        multilabel_dfs = []\n",
    "        for c in multilabel_cols:\n",
    "            list_srs = anime[c].map(lambda x: x.split(\", \")).tolist()\n",
    "            # MultiLabelBinarizerを使うと簡単に変換できるのでオススメです\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            ohe_srs = mlb.fit_transform(list_srs)\n",
    "            if c == \"genres\" or c == \"licensors\":\n",
    "                # ユニーク数が多くないのでOne-hot表現のまま\n",
    "                col_df = pd.DataFrame(ohe_srs, columns=[f\"ohe_{c}_{name}\" for name in mlb.classes_])\n",
    "            else:\n",
    "                # ユニーク数が多いので、SVDで次元圧縮する\n",
    "                svd = TruncatedSVD(n_components=10)\n",
    "                svd_arr = svd.fit_transform(ohe_srs)\n",
    "                col_df = pd.DataFrame(\n",
    "                    svd_arr,\n",
    "                    columns=[f\"svd_{c}_{ix}\" for ix in range(10)]\n",
    "                )\n",
    "            multilabel_dfs.append(col_df)\n",
    "\n",
    "        multilabel_df = pd.concat(multilabel_dfs, axis=1)\n",
    "        multilabel_df['anime_id'] = anime['anime_id'].to_numpy()\n",
    "        return multilabel_df\n",
    "    \n",
    "    def create_features(self, fold: int | None = None): \n",
    "        multilabel_cols = [\"genres\", \"producers\", \"licensors\", \"studios\"]\n",
    "        df = self.create_one_hot_and_svd_features(anime, multilabel_cols)\n",
    "        if fold is not None:\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _train = _train[['anime_id']].merge(df, on=\"anime_id\", how=\"left\")\n",
    "            _valid = _valid[['anime_id']].merge(df, on=\"anime_id\", how=\"left\")\n",
    "            self.train = _train.drop(columns=['anime_id'], axis=1)\n",
    "            self.valid = _valid.drop(columns=['anime_id'], axis=1)\n",
    "        else:\n",
    "            _train = train.copy()\n",
    "            _test = test.copy()\n",
    "            _train = _train[['anime_id']].merge(df, on=\"anime_id\", how=\"left\")\n",
    "            _test = _test[['anime_id']].merge(df, on=\"anime_id\", how=\"left\")\n",
    "            self.train = _train.drop(columns=['anime_id'], axis=1)\n",
    "            self.test = _test.drop(columns=['anime_id'], axis=1)\n",
    "\n",
    "\n",
    "class Anime2Vec(Feature):\n",
    "    @staticmethod\n",
    "    def create_model(title_sentence_list, vector_size):\n",
    "        \n",
    "        # ユーザごとにshuffleしたリストを作成\n",
    "        shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]\n",
    "\n",
    "        # 元のリストとshuffleしたリストを合わせる\n",
    "        train_sentence_list = title_sentence_list + shuffled_sentence_list\n",
    "        # train_sentence_list = title_sentence_list\n",
    "\n",
    "        # word2vecのパラメータ\n",
    "        w2v_params = {\n",
    "            \"vector_size\": vector_size,\n",
    "            \"seed\": c.seed,\n",
    "            # \"window\": 20,\n",
    "            \"min_count\": 1,\n",
    "            \"workers\": 1,\n",
    "            # \"epochs\": 20,\n",
    "        }\n",
    "\n",
    "        # word2vecのモデル学習\n",
    "        model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def create_embedding(model, user_anime_list_dict, anime_ids, user_emb_suffix, item_emb_suffix, vector_size):\n",
    "        \"\"\"\n",
    "        validation時: trainのみでembeddingを作成しtrainとvalidにmerge\n",
    "        submission時: trainでembeddingを作成しtrainとtestにmerge\n",
    "        \"\"\"\n",
    "        # ユーザーごとの特徴ベクトルと対応するユーザーID\n",
    "        user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n",
    "\n",
    "        # アイテムごとの特徴ベクトルと対応するアイテムID\n",
    "        item_factors = {aid: model.wv[aid] for aid in anime_ids}\n",
    "\n",
    "        # データフレームを作成\n",
    "        user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "        item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n",
    "        user_factors_df.columns = [\"user_id\"] + [f\"{i}_{user_emb_suffix}\" for i in range(vector_size)]\n",
    "        item_factors_df.columns = [\"anime_id\"] + [f\"{i}_{item_emb_suffix}\" for i in range(vector_size)]\n",
    "        return user_factors_df, item_factors_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sentence_list(_train:pd.DataFrame):\n",
    "        whole = _train.copy()\n",
    "        anime_ids = _train['anime_id'].unique().tolist()\n",
    "        user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in whole.groupby('user_id')['anime_id']}\n",
    "\n",
    "        # スコアを考慮する場合\n",
    "        # 今回は1～10のレーティングなので、スコアが5のアニメは5回、スコアが10のアニメは10回、タイトルをリストに追加する\n",
    "        title_sentence_list = []\n",
    "        for user_id, user_df in whole.groupby('user_id'):\n",
    "            user_title_sentence_list = []\n",
    "            for anime_id, anime_score in user_df[['anime_id', 'score']].values:\n",
    "                for i in range(int(anime_score)):\n",
    "                    user_title_sentence_list.append(anime_id)\n",
    "            title_sentence_list.append(user_title_sentence_list)\n",
    "\n",
    "        # whole = _train.copy()\n",
    "        # # scoreを元にembeddingを作成しているのでリークするかも\n",
    "        # title_sentence_list = whole.sort_values('score').groupby(['user_id'])['japanese_name'].unique().apply(lambda x: x.tolist()).to_list()\n",
    "        # # title_sentence_list = whole.groupby(['user_id', 'score'])['japanese_name'].unique().apply(lambda x: x.tolist()).to_list()\n",
    "        return title_sentence_list, anime_ids, user_anime_list_dict\n",
    "\n",
    "    # def create_features(self, fold: int | None = None):\n",
    "    #     pass\n",
    "    def create_features(self, fold:Optional[int] = None):\n",
    "        \"\"\" \n",
    "        train全体でembeddingを作成すると検証時にリークする(した)\n",
    "        →そこでcross validation時は分割後のtrainでembeddingを作成する\n",
    "\n",
    "        ただしtrain時とtest時でembeddingを変えると次元の意味が変わってしまう。\n",
    "        cvの場合fold=0で使われるembでtestを作成して予測,fold=1で使われるembでtestを作成して予測、とする必要がある\n",
    "        そのためにはtestの特徴量をfold分作成する必要がある\n",
    "\n",
    "        unseen userに対してはtrainで作成したuser_embにはvalidのuserは存在しないので全て欠損データとなるので除外\n",
    "        いや、やり方を工夫すれば使える\n",
    "        testを考えるとanime embはtrainのみを利用して作成しuser embはそれをtestに反映させる\n",
    "        となればuser embを作る際はtestのuserを含めても問題ない\n",
    "        \"\"\"\n",
    "        vector_size = 64\n",
    "        if fold is not None:\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _test = test.copy()\n",
    "            title_sentence_list, anime_ids, _ = self.create_sentence_list(_train)\n",
    "            # 使うanimeが分割後のtrainであればそれを元にuser embを作るのはtrain全体でもok\n",
    "            user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train.query('anime_id in @anime_ids').groupby('user_id')['anime_id']}\n",
    "            model = self.create_model(title_sentence_list, vector_size)\n",
    "            user_suffix = '_anime2vec_user_emb'\n",
    "            item_suffix = '_anime2vec_item_emb'\n",
    "            user_emb, item_emb = self.create_embedding(model, user_anime_list_dict, anime_ids, user_suffix, item_suffix, vector_size)\n",
    "\n",
    "\n",
    "            _train = _train.merge(user_emb, on='user_id', how='left')\n",
    "            _train = _train.merge(item_emb, on='anime_id', how='left')\n",
    "            _valid = _valid.merge(user_emb, on='user_id', how='left')\n",
    "            _valid = _valid.merge(item_emb, on='anime_id', how='left')\n",
    "            _test = _test.merge(user_emb, on='user_id', how='left')\n",
    "            _test = _test.merge(item_emb, on='anime_id', how='left')\n",
    "\n",
    "            self.train = _train.filter(like='_anime2vec_')\n",
    "            self.valid = _valid.filter(like='_anime2vec_')\n",
    "            self.test = _test.filter(like='_anime2vec_')\n",
    "        else:\n",
    "            _train = train.copy()\n",
    "            _test = test.copy()\n",
    "            _whole = pd.concat([_train, _test], axis=0)\n",
    "            title_sentence_list, anime_ids, _ = self.create_sentence_list(_train)\n",
    "            user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in _whole.query('anime_id in @anime_ids').groupby('user_id')['anime_id']}\n",
    "            model = self.create_model(title_sentence_list, vector_size)\n",
    "            user_suffix = '_anime2vec_user_emb'\n",
    "            item_suffix = '_anime2vec_item_emb'\n",
    "            user_emb, item_emb = self.create_embedding(model, user_anime_list_dict, anime_ids, user_suffix, item_suffix, vector_size)\n",
    "\n",
    "            _train = _train.merge(user_emb, on='user_id', how='left')\n",
    "            _train = _train.merge(item_emb, on='anime_id', how='left')\n",
    "            _test = _test.merge(user_emb, on='user_id', how='left')\n",
    "            _test = _test.merge(item_emb, on='anime_id', how='left')\n",
    "            self.train = _train.filter(like='_anime2vec_')\n",
    "            self.test = _test.filter(like='_anime2vec_')\n",
    "\n",
    "\n",
    "class Anime2VecWithoutScore(Feature):\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sentence_list(_whole:pd.DataFrame):\n",
    "        anime_ids = _whole['anime_id'].unique().tolist()\n",
    "        user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in _whole.groupby('user_id')['anime_id']}\n",
    "        title_sentence_list = _whole.groupby('user_id')['anime_id'].apply(list).tolist()\n",
    "        return title_sentence_list, anime_ids, user_anime_list_dict\n",
    "\n",
    "    def create_features(self, fold:Optional[int] = None):\n",
    "        \"\"\" \n",
    "        train全体でembeddingを作成すると検証時にリークする(した)\n",
    "        →そこでcross validationごとにembeddingを作成する\n",
    "\n",
    "        ただしtrain時とtest時でembeddingを変えると次元の意味が変わってしまう。\n",
    "        cvの場合fold=0で使われるembでtestを作成して予測,fold=1で使われるembでtestを作成して予測、とする必要がある\n",
    "        そのためにはtestの特徴量をfold分作成する必要がある\n",
    "\n",
    "        こちらはtest時には全てのデータを使った特徴量を作成する\n",
    "        よってvalid時はtrain+validで作成したembeddingを使う\n",
    "        unseen userを学習させる場合にも使える\n",
    "\n",
    "        改めて\n",
    "        embeddingを揃えるためにfoldごとにtestは作成する必要がある\n",
    "        ただし今回はtest含めて作っていいのでデータ全体でembeddingを作成する\n",
    "        validで若干リークしそう？(testを含めてるので)\n",
    "        \"\"\"\n",
    "\n",
    "        vector_size = 64\n",
    "        user_suffix = '_anime2vec_user_emb_wo_score'\n",
    "        item_suffix = '_anime2vec_item_emb_wo_score'\n",
    "        whole = pd.concat([train, test], axis=0)\n",
    "        title_sentence_list, anime_ids, user_anime_list_dict = self.create_sentence_list(whole)\n",
    "        model = Anime2Vec.create_model(title_sentence_list, vector_size)\n",
    "        user_emb, item_emb = Anime2Vec.create_embedding(model, user_anime_list_dict, anime_ids, user_suffix, item_suffix, vector_size)\n",
    "        # whole = whole.merge(user_emb, on='user_id', how='left')  # これするとなぜかindexがresetされる\n",
    "        _train = whole[~whole['score'].isna()].sort_index()\n",
    "        _test = whole[whole['score'].isna()].sort_index()\n",
    "        _train = _train.merge(user_emb, on='user_id', how='left')\n",
    "        _test = _test.merge(user_emb, on='user_id', how='left')\n",
    "\n",
    "        # itemは使わない？\n",
    "        if fold is not None:\n",
    "            _train, _valid = split_train_valid(_train, fold)\n",
    "            self.train = _train.filter(like='_wo_score')\n",
    "            self.valid = _valid.filter(like='_wo_score')\n",
    "            self.test = _test.filter(like='_wo_score')\n",
    "        else:\n",
    "            self.train = _train.filter(like='_wo_score')\n",
    "            self.test = _test.filter(like='_wo_score')\n",
    "\n",
    "\n",
    "class NumericalUserProfile(Feature):\n",
    "    def create_features(self, fold:Optional[int] = None):\n",
    "        \"\"\" \n",
    "        testを予測する際には全てのデータを使用する\n",
    "        よってvalid時はtrain+validで作成したデータを使用する\n",
    "\n",
    "        これはunseen userに対しても有効(test時にはtestを使っていいので)\n",
    "        \"\"\"\n",
    "    \n",
    "        if fold is not None:\n",
    "            user_features = train.groupby('user_id')[numerical_cols].agg(['mean', 'std', 'min', 'max'])\n",
    "            user_features.columns = user_features.columns.map('_'.join)\n",
    "            user_features.reset_index(inplace=True)\n",
    "\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _train = _train[['user_id']].merge(user_features, on='user_id', how='left')\n",
    "            _valid = _valid[['user_id']].merge(user_features, on='user_id', how='left')\n",
    "            self.train = _train.drop('user_id', axis=1)\n",
    "            self.valid = _valid.drop('user_id', axis=1)\n",
    "        else:\n",
    "            _whole = pd.concat([train, test]).reset_index(drop=True)  # animeはすでにmerge済み\n",
    "            user_features = _whole.groupby('user_id')[numerical_cols].agg(['mean', 'std', 'min', 'max'])\n",
    "            user_features.columns = user_features.columns.map('_'.join)\n",
    "            user_features.reset_index(inplace=True)\n",
    "\n",
    "            _train = train.copy()\n",
    "            _test = test.copy()\n",
    "            _train = _train[['user_id']].merge(user_features, on='user_id', how='left')\n",
    "            _test = _test[['user_id']].merge(user_features, on='user_id', how='left')\n",
    "            self.train = _train.drop('user_id', axis=1)\n",
    "            self.test = _test.drop('user_id', axis=1)\n",
    "\n",
    "\n",
    "class TfidfAllText(Feature):\n",
    "    def tfidf_svd_vectorize(self, input_df:pd.DataFrame, col:str, n_components:int = 50):\n",
    "        \"\"\"\n",
    "        usage:\n",
    "        tfidf_df = tfidf_svd_vectorize(df, col='abc', n_components=5)\n",
    "        次元圧縮したくない場合はpipelineのTruncatedSVDをコメントアウト\n",
    "        \"\"\"\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"TfidfVectorizer\", TfidfVectorizer()),\n",
    "            (\"TruncatedSVD\", TruncatedSVD(n_components=n_components, random_state=42))\n",
    "        ])\n",
    "        features = pipeline.fit_transform(input_df[col].fillna(\"\"))\n",
    "        output_df = pd.DataFrame(features).add_prefix(f'tfidf_svd_{col}_')\n",
    "        return output_df\n",
    "\n",
    "    def create_features(self, fold: int | None = None):\n",
    "\n",
    "        # 全体でやるとvalidでリークしない？\n",
    "        whole = pd.concat([train, test], axis=0)\n",
    "        whole['text'] = whole[['genres', 'aired', 'producers', 'licensors', 'studios', 'source', 'duration', 'rating']].apply(lambda x: ' '.join(x.values.astype(str)), axis=1)\n",
    "        user_df = whole.groupby('user_id')['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "        user_df['text'] = user_df['text'].str.replace(\",\", \"\")\n",
    "        user_emb_df = self.tfidf_svd_vectorize(user_df, 'text', n_components=50)\n",
    "        user_emb_df['user_id'] = user_df['user_id'].to_numpy()\n",
    "\n",
    "        if fold is not None:\n",
    "            # for cv\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _test = test.copy()\n",
    "            _train = _train.merge(user_emb_df, on='user_id', how='left')\n",
    "            _valid = _valid.merge(user_emb_df, on='user_id', how='left')\n",
    "            _test = _test.merge(user_emb_df, on='user_id', how='left')\n",
    "            self.train = _train.filter(like='tfidf_svd_')\n",
    "            self.valid = _valid.filter(like='tfidf_svd_')\n",
    "            self.test = _test.filter(like='tfidf_svd_')\n",
    "        else:\n",
    "            # for submission\n",
    "            _train = train.copy()\n",
    "            _test = test.copy()\n",
    "            _train = _train.merge(user_emb_df, on='user_id', how='left')\n",
    "            _test = _test.merge(user_emb_df, on='user_id', how='left')\n",
    "            self.train = _train.filter(like='tfidf_svd_')\n",
    "            self.test = _test.filter(like='tfidf_svd_')\n",
    "\n",
    "\n",
    "class TfidfEachText(Feature):\n",
    "    def tfidf_svd_vectorize(self, input_df:pd.DataFrame, col:str, n_components:int = 50):\n",
    "        \"\"\"\n",
    "        usage:\n",
    "        tfidf_df = tfidf_svd_vectorize(df, col='abc', n_components=5)\n",
    "        次元圧縮したくない場合はpipelineのTruncatedSVDをコメントアウト\n",
    "        \"\"\"\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"TfidfVectorizer\", TfidfVectorizer()),\n",
    "            (\"TruncatedSVD\", TruncatedSVD(n_components=n_components, random_state=42))\n",
    "        ])\n",
    "        features = pipeline.fit_transform(input_df[col].fillna(\"\"))\n",
    "        output_df = pd.DataFrame(features).add_prefix(f'tfidf_svd_{col}_')\n",
    "        return output_df\n",
    "\n",
    "    def create_features(self, fold: int | None = None):\n",
    "        \"\"\" \n",
    "        embeddingの次元の意味を揃えたいのでfoldごとにtestのデータを作成している\n",
    "        そうするとembeddingにはtestのデータも含めて作りたい\n",
    "        そうするとvalid時に本来は見ないtestも含まれることになるのでリークがある\n",
    "        \"\"\" \n",
    "        whole = pd.concat([train, test], axis=0)\n",
    "        user_emb_df = whole.groupby('user_id').size().reset_index().drop(0, axis=1)\n",
    "        whole = pd.concat([train, test], axis=0)\n",
    "        for col in ['genres', 'producers', 'licensors', 'studios']:\n",
    "            user_df = whole.groupby('user_id')[col].apply(lambda x: ' '.join(x)).reset_index()\n",
    "            user_df[col] = user_df[col].str.replace(\",\", \"\")\n",
    "            emb_df = self.tfidf_svd_vectorize(user_df, col, n_components=20)\n",
    "            emb_df['user_id'] = user_df['user_id'].to_numpy()\n",
    "            user_emb_df = user_emb_df.merge(emb_df, on='user_id', how='left')\n",
    "\n",
    "\n",
    "        if fold is not None:\n",
    "            # for cv\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _test = test.copy()\n",
    "            _train = _train.merge(user_emb_df, on='user_id', how='left')\n",
    "            _valid = _valid.merge(user_emb_df, on='user_id', how='left')\n",
    "            _test = _test.merge(user_emb_df, on='user_id', how='left')\n",
    "            self.train = _train.filter(like='tfidf_svd_')\n",
    "            self.valid = _valid.filter(like='tfidf_svd_')\n",
    "            self.test = _test.filter(like='tfidf_svd_')\n",
    "        else:\n",
    "            # for submission\n",
    "            _train = train.copy()\n",
    "            _test = test.copy()\n",
    "            _train = _train.merge(user_emb_df, on='user_id', how='left')\n",
    "            _test = _test.merge(user_emb_df, on='user_id', how='left')\n",
    "            self.train = _train.filter(like='tfidf_svd_')\n",
    "            self.test = _test.filter(like='tfidf_svd_')\n",
    "\n",
    "\n",
    "class Sentence2VecByUser(Feature):\n",
    "    def vectorize(self, x: str, ndim=160):\n",
    "        embeddings = [\n",
    "            self.model.get_vector(word)\n",
    "            if self.model.key_to_index.get(word, None) is not None\n",
    "            else np.zeros(ndim, dtype=np.float32)\n",
    "            for word in x.split()\n",
    "        ]\n",
    "        if len(embeddings) == 0:\n",
    "            return np.zeros(ndim, dtype=np.float32)\n",
    "        else:\n",
    "            return np.mean(embeddings, axis=0)\n",
    "\n",
    "    def vectorize_to_df(self,input_df, col, prefix, ndim=160):\n",
    "        vector = np.stack(\n",
    "            input_df[col].fillna(\"\").str.replace(\"\\n\", \"\").progress_apply(lambda x: self.vectorize(x, ndim)).to_numpy()\n",
    "            )\n",
    "        output_df = pd.DataFrame(vector).add_prefix('senentce2vec_')\n",
    "        return output_df \n",
    "\n",
    "    def create_features(self, fold: int | None = None):\n",
    "        \"\"\" \n",
    "        embeddinの次元を揃えるためにtestはfoldごとに作成する\n",
    "        そのためuser embはtrainとtestをconcatして作成する\n",
    "        \"\"\"\n",
    "        self.model = gensim.downloader.load('glove-wiki-gigaword-50')\n",
    "        vector_size = self.model.vector_size\n",
    "        # 全体でやるとvalidでリークしない？\n",
    "        whole = pd.concat([train, test], axis=0)\n",
    "        whole['text'] = whole[['genres', 'aired', 'producers', 'licensors', 'studios', 'source', 'duration', 'rating']].apply(lambda x: ' '.join(x.values.astype(str)), axis=1)\n",
    "        user_df = whole.groupby('user_id')['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "        user_df['text'] = user_df['text'].str.replace(\",\", \"\")\n",
    "\n",
    "        prefix = 'senentce2vec'\n",
    "        user_emb_df = self.vectorize_to_df(user_df, 'text', prefix, ndim=vector_size)\n",
    "        user_emb_df['user_id'] = user_df['user_id'].to_numpy()\n",
    "\n",
    "        if fold is not None:\n",
    "            # for cv\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _test = test.copy()\n",
    "            _train = _train.merge(user_emb_df, on='user_id', how='left')\n",
    "            _valid = _valid.merge(user_emb_df, on='user_id', how='left')\n",
    "            _test = _test.merge(user_emb_df, on='user_id', how='left')\n",
    "            self.train = _train.filter(like=prefix)\n",
    "            self.valid = _valid.filter(like=prefix)\n",
    "            self.test = _test.filter(like=prefix)\n",
    "        else:\n",
    "            # for submission\n",
    "            _train = train.copy()\n",
    "            _test = test.copy()\n",
    "            _train = _train.merge(user_emb_df, on='user_id', how='left')\n",
    "            _test = _test.merge(user_emb_df, on='user_id', how='left')\n",
    "            self.train = _train.filter(like=prefix)\n",
    "            self.test = _test.filter(like=prefix)\n",
    "\n",
    "\n",
    "class SVDppEmb(Feature):\n",
    "    def get_user_emb(self, _train):\n",
    "        user_emb = {}\n",
    "        for user_id in _train['user_id'].unique():\n",
    "            user_emb[user_id] = self.algo.pu[self.algo.trainset.to_inner_uid(user_id)]\n",
    "        user_emb = pd.DataFrame(user_emb).T.add_prefix('svdpp_user_emb_').reset_index().rename(columns={'index': 'user_id'})\n",
    "        return user_emb\n",
    "    \n",
    "    def get_anime_emb(self, _train):\n",
    "        anime_emb = {}\n",
    "        for anime_id in _train['anime_id'].unique():\n",
    "            anime_emb[anime_id] = self.algo.qi[self.algo.trainset.to_inner_iid(anime_id)]\n",
    "        anime_emb = pd.DataFrame(anime_emb).T.add_prefix('svdpp_anime_emb_').reset_index().rename(columns={'index': 'anime_id'})\n",
    "        return anime_emb\n",
    "\n",
    "    def create_features(self, fold: int | None = None):\n",
    "        if fold is not None:\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _test = test.copy()\n",
    "            reader = Reader(rating_scale=(1, 10))\n",
    "            train_data = Dataset.load_from_df(_train[['user_id', 'anime_id', 'score']], reader).build_full_trainset()\n",
    "            self.algo = SVDpp()\n",
    "            self.algo.fit(train_data)\n",
    "\n",
    "            if c.cv_strategy != 'group':\n",
    "                user_emb = self.get_user_emb(_train)\n",
    "                _train = _train.merge(user_emb, on='user_id', how='left')\n",
    "                _valid = _valid.merge(user_emb, on='user_id', how='left')\n",
    "                _test = _test.merge(user_emb, on='user_id', how='left')\n",
    "    \n",
    "            # anime_emb = self.get_anime_emb(_train)\n",
    "            # _train = _train.merge(anime_emb, on='anime_id', how='left')\n",
    "            # _valid = _valid.merge(anime_emb, on='anime_id', how='left')\n",
    "            # _test = _test.merge(anime_emb, on='anime_id', how='left')\n",
    "\n",
    "            self.train = _train.filter(like='svdpp')\n",
    "            self.valid = _valid.filter(like='svdpp')       \n",
    "            self.test = _test.filter(like='svdpp')\n",
    "        else:\n",
    "            _train = train.copy()\n",
    "            _test = test.copy()\n",
    "            reader = Reader(rating_scale=(1, 10))\n",
    "            train_data = Dataset.load_from_df(_train[['user_id', 'anime_id', 'score']], reader).build_full_trainset()\n",
    "            self.algo = SVDpp()\n",
    "            self.algo.fit(train_data)\n",
    "\n",
    "            if c.cv_strategy != 'group':\n",
    "                user_emb = self.get_user_emb(_train)\n",
    "                _train = _train.merge(user_emb, on='user_id', how='left')\n",
    "                _test = _test.merge(user_emb, on='user_id', how='left')\n",
    "\n",
    "            # anime_emb = self.get_anime_emb(_train)\n",
    "            # _train = _train.merge(anime_emb, on='anime_id', how='left')\n",
    "            # _test = _test.merge(anime_emb, on='anime_id', how='left')\n",
    "            self.train = _train.filter(like='svdpp')\n",
    "            self.test = _test.filter(like='svdpp')\n",
    "\n",
    "\n",
    "class KNNBaselineSimEmb(Feature):\n",
    "    def get_user_emb(self, _train):\n",
    "        reader = Reader(rating_scale=(1, 10))\n",
    "        trainset = Dataset.load_from_df(_train[['user_id', 'anime_id', 'score']], reader).build_full_trainset()\n",
    "        algo = KNNBaseline(sim_options={'name': 'pearson_baseline', 'user_based': True})\n",
    "        algo.fit(trainset)\n",
    "\n",
    "        pca = PCA(n_components=50, random_state=0)\n",
    "        reduced_matrix = pca.fit_transform(algo.sim)\n",
    "        user_sim = pd.DataFrame(reduced_matrix).add_prefix('knn_baseline_user')\n",
    "        user_sim['user_id'] = _train['user_id'].unique()  # uniqueの順で並んでいることを確認\n",
    "        return user_sim\n",
    "\n",
    "    def get_anime_emb(self, _train):\n",
    "        reader = Reader(rating_scale=(1, 10))\n",
    "        trainset = Dataset.load_from_df(_train[['user_id', 'anime_id', 'score']], reader).build_full_trainset()\n",
    "        algo = KNNBaseline(sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "        algo.fit(trainset)\n",
    "        \n",
    "        pca = PCA(n_components=50, random_state=0)\n",
    "        reduced_matrix = pca.fit_transform(algo.sim)\n",
    "        anime_sim = pd.DataFrame(reduced_matrix).add_prefix('knn_baseline_item')\n",
    "        anime_sim['anime_id'] = _train['anime_id'].unique()  # uniqueの順で並んでいることを確認\n",
    "        return anime_sim\n",
    "    \n",
    "\n",
    "    def create_features(self, fold: int | None = None):\n",
    "        if fold is not None:\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _test = test.copy()\n",
    "            \n",
    "            if c.cv_strategy != 'group':\n",
    "                user_emb = self.get_user_emb(_train)\n",
    "                _train = _train.merge(user_emb, on='user_id', how='left')\n",
    "                _valid = _valid.merge(user_emb, on='user_id', how='left')\n",
    "                _test = _test.merge(user_emb, on='user_id', how='left')\n",
    "\n",
    "            anime_emb = self.get_anime_emb(_train)\n",
    "            _train = _train.merge(anime_emb, on='anime_id', how='left')\n",
    "            _valid = _valid.merge(anime_emb, on='anime_id', how='left')\n",
    "            _test = _test.merge(anime_emb, on='anime_id', how='left')\n",
    "            self.train = _train.filter(like='knn_baseline')\n",
    "            self.valid = _valid.filter(like='knn_baseline')\n",
    "            self.test = _test.filter(like='knn_baseline')\n",
    "        else:\n",
    "            _train, _test = train.copy(), test.copy()\n",
    "            if c.cv_strategy != 'group':\n",
    "                user_emb = self.get_user_emb(_train)\n",
    "                _train = _train.merge(user_emb, on='user_id', how='left')\n",
    "                _test = _test.merge(user_emb, on='user_id', how='left')\n",
    "\n",
    "            anime_emb = self.get_anime_emb(_train)\n",
    "            _train = _train.merge(anime_emb, on='anime_id', how='left')\n",
    "            _test = _test.merge(anime_emb, on='anime_id', how='left')\n",
    "            self.train = _train.filter(like='knn_baseline')\n",
    "            self.test = _test.filter(like='knn_baseline')\n",
    "\n",
    "\n",
    "class NumericalCatProfile(Feature):\n",
    "    def aggregate_by_anime(self, anime):\n",
    "        # multilabel_cols = ['genres']\n",
    "        # df = MultiTagEncode.create_one_hot_and_svd_features(anime, multilabel_cols)\n",
    "        # target_cols = list(df.columns)\n",
    "        # target_cols += ['original_work_name', 'type', 'source', 'rating']\n",
    "        # _anime = anime.merge(df, on='anime_id', how='left')\n",
    "\n",
    "        # 上のやり方だと数が多いので一旦下でやる\n",
    "        _anime = anime.copy()\n",
    "        target_cols = ['original_work_name', 'type', 'source', 'rating']\n",
    "\n",
    "        for col in target_cols:\n",
    "            gr = _anime.groupby(col)[numerical_cols].agg(['mean', 'std', 'min', 'max'])\n",
    "            gr.columns = gr.columns.map('_'.join)\n",
    "            gr = gr.add_suffix(f'_{col}_agg')\n",
    "            _anime = _anime.merge(gr, on=col, how='left')\n",
    "        _anime = _anime.filter(like='_agg')\n",
    "        _anime['anime_id'] = anime['anime_id'].to_numpy()\n",
    "        return _anime\n",
    "\n",
    "    def create_features(self, fold:Optional[int] = None):\n",
    "        _anime = self.aggregate_by_anime(anime)\n",
    "        if fold is not None:\n",
    "            _train, _valid = split_train_valid(train, fold)\n",
    "            _train = _train[['anime_id']].merge(_anime, on='anime_id', how='left')\n",
    "            _valid = _valid[['anime_id']].merge(_anime, on='anime_id', how='left')\n",
    "            self.train = _train.drop('anime_id', axis=1)\n",
    "            self.valid = _valid.drop('anime_id', axis=1)\n",
    "        else:\n",
    "            _train = train.copy()\n",
    "            _test = test.copy()\n",
    "            _train = _train[['anime_id']].merge(_anime, on='anime_id', how='left')\n",
    "            _test = _test[['anime_id']].merge(_anime, on='anime_id', how='left')\n",
    "            self.train = _train.drop('anime_id', axis=1)\n",
    "            self.test = _test.drop('anime_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_features(globals(), overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\n",
    "    \"Numerical\",\n",
    "    \"OrdinalEncode\",\n",
    "    \"CountEncode\",\n",
    "    \"TargetEncode\",\n",
    "    \"Anime2Vec\",\n",
    "    \"MultiTagEncode\",\n",
    "    \"NumericalUserProfile\",\n",
    "    \"Anime2VecWithoutScore\",\n",
    "    \"TfidfAllText\",\n",
    "    \"TfidfEachText\",\n",
    "    \"Sentence2VecByUser\",\n",
    "    \"SVDppEmb\",\n",
    "    \"KNNBaselineSimEmb\",\n",
    "    \"NumericalCatProfile\",\n",
    "]\n",
    "\n",
    "\n",
    "targets = [\n",
    "    \"Target\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(feats, fold=0):\n",
    "    \"\"\" \n",
    "    1 foldだけで動作確認する\n",
    "    \"\"\"\n",
    "    not_feature_cols = [\n",
    "        c.target_col,\n",
    "        'fold'\n",
    "    ]\n",
    "    train_feats = load_datasets(feats, input_dir=c.feature_dir, phase='train', fold=fold)\n",
    "    valid_feats = load_datasets(feats, input_dir=c.feature_dir, phase='valid', fold=fold)\n",
    "    test_feats = load_datasets(feats, input_dir=c.feature_dir, phase='test', fold=fold)\n",
    "\n",
    "    train_target = load_datasets(targets, input_dir=c.feature_dir, phase='train', fold=fold)\n",
    "    valid_target = load_datasets(targets, input_dir=c.feature_dir, phase='valid', fold=fold)\n",
    "\n",
    "    # 使用する特徴量&label\n",
    "    print(\"##################\")\n",
    "    print('features')\n",
    "    print(\"##################\")\n",
    "    for f in train_feats.columns:\n",
    "        print(f)\n",
    "        assert f not in not_feature_cols\n",
    "\n",
    "    print(f'train:{train_feats.shape}')\n",
    "    print(f'valid:{valid_feats.shape}')\n",
    "    print(f'test:{test_feats.shape}')\n",
    "    assert train_feats.shape[1] == valid_feats.shape[1] == test_feats.shape[1]\n",
    "    assert train_feats.shape[0] == train_target.shape[0]\n",
    "    assert valid_feats.shape[0] == valid_target.shape[0]\n",
    "    assert test_feats.shape[0] == test.shape[0]\n",
    "    print(\"##################\")\n",
    "    print('categorical features')\n",
    "    print(\"##################\")\n",
    "    cat_cols = get_categorical_col(train_feats)\n",
    "    for col in cat_cols:\n",
    "        print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(feats, fold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.gbdt import get_callbacks\n",
    "from lightgbm import register_logger\n",
    "register_logger(logger)\n",
    "callbacks = get_callbacks(c.model_name)\n",
    "callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.gbdt import get_model # , LGBModel, XGBModel, CBModel \n",
    "sample_data = load_datasets(feats, input_dir=c.feature_dir, phase='train', fold=None)\n",
    "cat_cols = get_categorical_col(sample_data)\n",
    "model = get_model(c.model_name, c.model_params, c.num_boost_round, cat_cols, c.output_dir, callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, mean_squared_error\n",
    "def calc_score(true, pred):\n",
    "    return mean_squared_error(true, pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oofs = []\n",
    "preds = []\n",
    "scores = []\n",
    "for fold in c.use_fold:\n",
    "    idx_train, idx_valid = cv[fold]\n",
    "    wandb.init(**c.wandb_init, name=f'exp{c.version}-fold{fold}')\n",
    "\n",
    "    X_train = load_datasets(feats, input_dir=c.feature_dir, phase='train', fold=fold)\n",
    "    X_valid = load_datasets(feats, input_dir=c.feature_dir, phase='valid', fold=fold)\n",
    "    X_test = load_datasets(feats, input_dir=c.feature_dir, phase='test', fold=fold)\n",
    "\n",
    "    y_train = load_datasets(targets, input_dir=c.feature_dir, phase='train', fold=fold)\n",
    "    y_valid = load_datasets(targets, input_dir=c.feature_dir, phase='valid', fold=fold)\n",
    "\n",
    "    model.train(X_train, y_train, X_valid, y_valid)\n",
    "    model.save(fold)\n",
    "    pred = model.predict(X_valid)\n",
    "\n",
    "    # evaluate\n",
    "    score = calc_score(y_valid, pred)\n",
    "    logger.info(f'fold-{fold} score: {score}')\n",
    "    wandb.log({'CV': score})\n",
    "    scores.append(score)\n",
    "\n",
    "    # create oof\n",
    "    oof_df = pd.DataFrame(pred, index=idx_valid)\n",
    "    oofs.append(oof_df)\n",
    "\n",
    "    # pred\n",
    "    pred_test = model.predict(X_test)\n",
    "    np.save(c.output_dir / f\"pred_test_{fold}\", pred_test)\n",
    "    preds.append(pred_test)\n",
    "\n",
    "    if fold!=c.use_fold[-1]:\n",
    "        wandb.finish()\n",
    "\n",
    "# oofを保存\n",
    "total_score = np.mean(scores)\n",
    "logger.info(f'total score: {total_score}')\n",
    "wandb.log({'TotalCV': total_score})\n",
    "oof = np.array(pd.concat(oofs).sort_index())\n",
    "np.save(c.output_dir / \"oof\", oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import plot_importance\n",
    "# catboostは対応していない\n",
    "plot_importance(model.models, output_dir=c.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds = []\n",
    "for i in range(len(cv)):\n",
    "    pred = np.load(f'{c.output_dir}/pred_test_{i}.npy')\n",
    "    preds.append(pred)\n",
    "pred_test = np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.load(f'{c.output_dir}/oof.npy').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if c.cv_strategy == 'group':\n",
    "    idx = test['user_id'].isin(test_only_user_ids)\n",
    "else:\n",
    "    idx = ~test['user_id'].isin(test_only_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(train[c.target_col], label='train', color='blue', alpha=0.5, bins=50, kde=True)\n",
    "sns.histplot(oof, label='oof', color='red', alpha=0.5, bins=50, kde=True)\n",
    "sns.histplot(pred_test[idx], label='test', color='orange', alpha=0.5, bins=50, kde=True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sub = pd.read_csv(c.input_dir / 'sample_submission.csv')\n",
    "assert sub.shape[0] == pred_test.shape[0]\n",
    "sub['score'] = pred_test\n",
    "sub.to_csv(c.output_dir / f'submission_exp{c.version}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sub = pd.read_csv('../output/030/submission_exp030.csv')\n",
    "sub1 = pd.read_csv(f'../output/{c.version}/group/submission_exp{c.version}.csv')\n",
    "sub2 = pd.read_csv(f'../output/{c.version}/stratified/submission_exp{c.version}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test = pd.read_csv(c.input_dir / 'test.csv')\n",
    "_test['score'] = sub2['score'].to_numpy()\n",
    "\n",
    "# testのみに存在するuserに対してはgroupkfoldの予測値を使う\n",
    "idx = _test['user_id'].isin(test_only_user_ids)\n",
    "_test.loc[idx, 'score'] = sub1.loc[idx, 'score'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = _test[['score']]\n",
    "# 下限を1に,上限を10にclipする\n",
    "sub['score'] = sub['score'].clip(1, 10)\n",
    "sub.to_csv(c.output_root_dir / f'submission_exp{c.version}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(base_sub['score'], sub['score'], alpha=0.1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
