{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/columbia2131/training-inference-code-xlm-roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys \n",
    "import os \n",
    "import logzero \n",
    "import wandb \n",
    "import pickle \n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config():\n",
    "    # common\n",
    "    version = '008'\n",
    "    comment = 'test'\n",
    "    input_dir = '/home/user/work/input/we-are-all-alike-on-the-inside'\n",
    "    output_dir = f'/home/user/work/output/{version}' \n",
    "    seed = 42\n",
    "    debug = False \n",
    "    target_col = None \n",
    "\n",
    "    # wandb\n",
    "    wandb_init = {\n",
    "        \"project\": \"debug\",\n",
    "        \"entity\": \"kuto5046\",\n",
    "        \"group\": f\"exp{version}\",\n",
    "        \"dir\": output_dir,\n",
    "        \"tags\": [],\n",
    "        \"mode\": \"disabled\", \n",
    "    }\n",
    "\n",
    "    # cv \n",
    "    n_splits = 5\n",
    "    use_fold = [0]  # fold1つで終える場合[0], 全てのfoldを実行する場合[0,1,2,3,4]\n",
    "\n",
    "    # dataloader\n",
    "    loader_params = {\n",
    "        \"train\": {'batch_size': 32, 'shuffle': True, 'num_workers': 4},\n",
    "        \"valid\": {'batch_size': 32, 'shuffle': False, 'num_workers': 4},\n",
    "        \"test\": {'batch_size': 32, 'shuffle': False, 'num_workers': 4} \n",
    "        }\n",
    "\n",
    "    # model\n",
    "    # res\n",
    "    resume_checkpoint_path = None #f\"{output_dir}/model_fold0_epoch=0.ckpt\"  # resume用\n",
    "    # pretrained_model_path = f\"{output_dir}/model_fold0_epoch=0.ckpt\"  # 予測のみ用 \n",
    "    n_epochs = 1\n",
    "    model_name = 'xlm-roberta-base'\n",
    "    max_len = 128\n",
    "    weight_decay = 1e-3\n",
    "    beta = (0.9, 0.98)\n",
    "    lr = 3e-5\n",
    "    num_warmup_steps_rate = 0\n",
    "    gradient_accumulation_steps = 1  # 1なら累積しない\n",
    "\n",
    "c = Config()\n",
    "DEBUG = c.debug \n",
    "# c = HydraConfig.get_cnf(config_path='/home/user/work/configs/', config_name='config.yaml')\n",
    "os.makedirs(c.output_dir, exist_ok=True)\n",
    "logger = logzero.setup_logger(name='main', logfile=f'{c.output_dir}/result.log', level=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "c.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(c.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{c.input_dir}/train.csv')\n",
    "test = pd.read_csv(f'{c.input_dir}/test.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.base import get_categorical_col, get_numerical_col\n",
    "from src.features.encoder import pp_for_categorical_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.target_col = 'category'\n",
    "c.n_class = 3\n",
    "c.target_map = {'association': 0, 'disagreement': 1, 'unbiased': 2}\n",
    "c.target_map_rev = {0: 'association', 1: 'disagreement', 2: 'unbiased'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_categorical_col(whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_numerical_col(whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast \n",
    "# def fix_s1s2(data):\n",
    "#     new_s1 = []\n",
    "#     new_s2 = []\n",
    "#     for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "#         if row[\"s1\"].startswith(\"[\"):\n",
    "#             try:\n",
    "#                 temp_s1 = \" \".join(ast.literal_eval(row[\"s1\"]))\n",
    "#             except SyntaxError:\n",
    "#                 temp_s1 = row[\"s1\"][1:-1]\n",
    "#         else:\n",
    "#             temp_s1 = row[\"s1\"]\n",
    "\n",
    "#         if row[\"s2\"].startswith(\"[\"):\n",
    "#             try:\n",
    "#                 temp_s2 = \" \".join(ast.literal_eval(row[\"s2\"]))\n",
    "#             except SyntaxError:\n",
    "#                 temp_s2 = row[\"s2\"][1:-1]\n",
    "#         else:\n",
    "#             temp_s2 = row[\"s2\"]\n",
    "\n",
    "#         new_s1.append(temp_s1)\n",
    "#         new_s2.append(temp_s2)\n",
    "#     data[\"s1\"] = new_s1\n",
    "#     data[\"s2\"] = new_s2\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole['s1'] = whole['s1'].map(\n",
    "    lambda x: x\\\n",
    "        .replace(\"['\", '')\\\n",
    "        .replace(\"']\", '')\\\n",
    "        .replace('[\"', '')\\\n",
    "        .replace('\"]', '')\\\n",
    "        .replace('[«', '«')\n",
    "        .replace('»]', '»')\\\n",
    "        .replace('[', '')\\\n",
    "        .replace(']', '')\\\n",
    "        .split(\"', '\")\n",
    ")\n",
    "\n",
    "\n",
    "whole['s1'] = whole['s1'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = whole[~whole[c.target_col].isna()].reset_index(drop=True)\n",
    "test = whole[whole[c.target_col].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelを数値に変換\n",
    "train[c.target_col] = train[c.target_col].map(c.target_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, config: Config, phase: str='train'):\n",
    "        assert phase in ['train', 'valid', 'test']\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        self.phase = phase\n",
    "        self.s1 = df['s1'].to_numpy()\n",
    "        self.s2 = df['s2'].to_numpy()\n",
    "        self.y = np.full(len(df), np.nan)\n",
    "        if self.phase in ['train', 'valid']:\n",
    "            self.y = df[config.target_col].to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.s1.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        inputs1 = self.tokenizer.encode_plus(\n",
    "            self.s1[idx],\n",
    "            self.s2[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.config.max_len, \n",
    "            padding='max_length',\n",
    "            truncation=True, \n",
    "            # return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        x = {\n",
    "            'token1': torch.tensor(inputs1['input_ids'], dtype=torch.long),\n",
    "            'mask1': torch.tensor(inputs1['attention_mask'], dtype=torch.long),\n",
    "        }\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name, n_class):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.ln = nn.LayerNorm(768)\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_class) \n",
    "        )\n",
    "\n",
    "    # @torch.autocast()\n",
    "    def forward(self, x):\n",
    "        output = self.backbone(x['token1'], attention_mask=x['mask1'])[\"last_hidden_state\"][:, 0, :]\n",
    "        output = self.ln(output)\n",
    "        output = self.linear1(output)        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "class CustomTask(pl.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = self.get_criterion(config)\n",
    "        self.optimizer = self.get_optimizer(config)\n",
    "        self.scheduler = self.get_scheduler(config)\n",
    "        self.metric = self.get_metric(config)\n",
    "\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        loss = self.criterion(output, y)\n",
    "        score = self.metric(output, y) \n",
    "\n",
    "        self.log(f'Loss/{mode}', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'Score/{mode}', score, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        return loss \n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode=\"train\")\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode=\"valid\")\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler, \"monitor\": \"Loss/valid\"}\n",
    "\n",
    "\n",
    "    def get_metric(self, config):\n",
    "        return F1Score(average='micro')\n",
    "\n",
    "\n",
    "    def get_optimizer(self, config: dict):\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']  # このパラメータはweight decayしない\n",
    "        optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "                    'weight_decay': config.weight_decay\n",
    "                },\n",
    "                {\n",
    "                    'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "                    'weight_decay': 0.0\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr = config.lr,\n",
    "            betas = config.beta,\n",
    "            weight_decay = config.weight_decay,\n",
    "            )\n",
    "        return optimizer\n",
    "\n",
    "    def get_scheduler(self, config: dict):\n",
    "        num_train_optimization_steps = int(\n",
    "            config.len_loader * config.n_epochs // config.gradient_accumulation_steps\n",
    "        )\n",
    "        num_warmup_steps = int(num_train_optimization_steps * config.num_warmup_steps_rate)\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_train_optimization_steps\n",
    "        )\n",
    "        return scheduler \n",
    "\n",
    "\n",
    "    def get_criterion(self, config: dict):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cv import get_kfold, get_stratifiedkfold, get_groupkfold\n",
    "cv = get_stratifiedkfold(train, c.target_col, n_splits=5)\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "def calc_score(true, pred):\n",
    "    return f1_score(true, pred.argmax(axis=1), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_device_to_dict(_dict, device):\n",
    "    for k, v in _dict.items():\n",
    "        _dict[k] = v.to(device)\n",
    "    return _dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(input):\n",
    "    return input.detach().cpu().numpy()\n",
    "\n",
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "def inference(model, loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        # https://github.com/tqdm/tqdm/issues/746\n",
    "        for batch in tqdm(loader, total=len(loader)):\n",
    "            with torch.autocast(device_type=device.type):\n",
    "                x, y = batch\n",
    "                x = apply_device_to_dict(x, device)\n",
    "                output = model(x)\n",
    "                pred.append(to_np(output))\n",
    "    return np.concatenate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(train, test, cv, config, target_col):\n",
    "    # 関数で実行するとdebugしにくいのでそのまま実行する\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv):\n",
    "        if i not in c.use_fold:\n",
    "            continue \n",
    "\n",
    "        wandb.init(**config.wandb_init, name=f'exp{config.version}-fold{i}')\n",
    "\n",
    "        _train = train.loc[idx_train].reset_index(drop=True)\n",
    "        _valid = train.loc[idx_valid].reset_index(drop=True)\n",
    "\n",
    "        loaders = {}\n",
    "        loaders[\"train\"] = DataLoader(CustomDataset(_train, config, phase=\"train\"), **config.loader_params['train'], worker_init_fn=worker_init_fn) \n",
    "        loaders[\"valid\"] = DataLoader(CustomDataset(_valid, config, phase=\"valid\"), **config.loader_params['valid'], worker_init_fn=worker_init_fn)\n",
    "        loaders[\"test\"] = DataLoader(CustomDataset(test, config, phase=\"test\"), **config.loader_params['test'], worker_init_fn=worker_init_fn)\n",
    "\n",
    "        c.len_loader = len(loaders['train'])\n",
    "\n",
    "        model = CustomModel(c.model_name, c.n_class)\n",
    "        task = CustomTask(model, c)\n",
    "\n",
    "        # callback \n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=f'Score/valid',\n",
    "            mode='max',\n",
    "            dirpath=c.output_dir,\n",
    "            verbose=True,\n",
    "            filename=f'model_fold{i}_' + '{epoch}')  # pl内部のepochを読む\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor='Loss/valid',\n",
    "            min_delta=0.00,\n",
    "            patience=3,\n",
    "            verbose=True,\n",
    "            mode='min')\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            logger=[WandbLogger()], \n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            max_epochs=c.n_epochs,\n",
    "            devices='auto',\n",
    "            accelerator='auto',\n",
    "            fast_dev_run=DEBUG,\n",
    "            deterministic=True,\n",
    "            precision=16,\n",
    "            )\n",
    "\n",
    "        print('start train')\n",
    "        # if os.path.exists(c.pretrained_model_path):\n",
    "        #     logger.info(f'load pretrained model {c.pretrained_model_path} and skip train')\n",
    "        #     checkpoint = torch.load(c.pretrained_model_path)\n",
    "        #     model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        # else:\n",
    "        trainer.fit(task, train_dataloaders=loaders['train'], val_dataloaders=loaders['valid'], ckpt_path=c.resume_checkpoint_path) # resumeする場合ここにcheckpointを渡す\n",
    "        if not DEBUG:\n",
    "            best_checkpoint = checkpoint_callback.best_model_path\n",
    "            logger.info(f'load best model {best_checkpoint}')\n",
    "            checkpoint = torch.load(best_checkpoint)\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
    "            config.best_checkpoint = best_checkpoint \n",
    "\n",
    "        print('create oof')\n",
    "        pred = inference(model, loaders['valid'], c.device)\n",
    "        oof = pd.DataFrame(pred, index=idx_valid)\n",
    "        oof.to_csv(f\"{c.output_dir}/oof_{i}.csv\", index=True) # もとの並びでconcatするときにindexが必要\n",
    "\n",
    "        # evaluate\n",
    "        print('evaluate valid data')\n",
    "        score = calc_score(_valid[c.target_col], pred)\n",
    "        logger.info(f'fold-{i} score: {score}')\n",
    "        wandb.log({'CV': score})\n",
    "\n",
    "        # pred\n",
    "        print('inference test data')\n",
    "        pred_test = inference(model, loaders['test'], c.device)\n",
    "        np.save(f\"{c.output_dir}/pred_test_{i}\", pred_test)\n",
    "\n",
    "        # if i != c.use_fold[-1]:\n",
    "        #     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline(train, test, cv, c, c.target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.best_checkpoint = f'{c.output_dir}/model_fold0_epoch=0.ckpt'\n",
    "model = CustomModel(c.model_name, c.n_class)\n",
    "checkpoint = torch.load(c.best_checkpoint)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (idx_train, idx_valid) in enumerate(cv):\n",
    "    break \n",
    "\n",
    "_valid = train.loc[idx_valid].reset_index(drop=True)\n",
    "\n",
    "loaders = {}\n",
    "loaders[\"valid\"] = DataLoader(CustomDataset(_valid, c, phase=\"valid\"), **c.loader_params['valid'], worker_init_fn=worker_init_fn)\n",
    "loaders[\"test\"] = DataLoader(CustomDataset(test, c, phase=\"test\"), **c.loader_params['test'], worker_init_fn=worker_init_fn)\n",
    "pred_valid = inference(model, loaders['valid'], c.device)\n",
    "y_valid = _valid[c.target_col].to_numpy()\n",
    "calc_score(y_valid, pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(len(cv)):\n",
    "    # TODO 存在していればに変更\n",
    "    pred = np.load(f'{c.output_dir}/pred_test_{i}.npy')\n",
    "    preds.append(pred)\n",
    "pred_test = np.mean(preds, axis=0).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train[c.target_col], label='target')\n",
    "sns.distplot(pred_test, label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(f'{c.input_dir}/sample_submission.csv')\n",
    "sub[c.target_col] = pred_test\n",
    "sub.to_csv(f'{c.output_dir}/submission_exp{c.version}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
